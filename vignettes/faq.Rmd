---
title: "Frequently Asked Questions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Frequently Asked Questions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# How does `multibias` fit into the causal inference workflow?

If the causal analysis process involves the following general steps:

1. Specify a causal question
2. Graph the causal assumptions (via a DAG)
3. Apply confounder balancing (matching, weighting, etc.)
4. Assess confounder balance
5. Estimate the causal effect
6. Perform sensitivity analysis of the effect estimate

Then `multibias` plugs into the final step of running sensitivity analyses.
Traditionally, the sensitivity analyses involve re-estimating results under
different assumptions related to the specified or unspecified confounders via
technique such as E-values and tipping point analyses. However, we know that
confounding is just one of several different types of biases that may impact
causal estimates, and `multibias` helps fill this void. It does so by allowing
for the simultaneous adjustment of **all** biases: uncontrolled confounding,
exposure misclassification, outcome misclassification, and selection bias. This
allows for a more transparent and complete understanding of how **all**
potential sources of systematic error may be impacting the analysis.

# My causal analysis leveraged a propensity score - how does that fit into a `multibias` analysis?

asdf

# What is bootstrapping and why does that matter here?

Bootstrapping is a statistical technique used to estimate the uncertainty
around an estimate. Instead of analyzing your study data once, bootstrapping
involves repeatedly drawing random samples with replacement from your original
data. Each of these samples is the same size as the original data. For each
bootstrap sample, you then calculate the statistic of interest - a bias-adjusted
estimate in the context of `multibias`. By repeating this resampling,
you end up with a distribution of the statistic. From this distribution you can
derive the confidence interval and standard error.

When `multibias` adjusts the data's exposure-outcome effect for potential
biases, the adjusted estimate has uncertainty. This uncertainty comes not only
from the original random (sampling) error in the data, but also from the
uncertainty in the bias parameters (systematic error). **Bootstrapping is particularly important for `multibias` because it allows for confidence intervals that incorporate two sources of uncertainty: uncertainty due to random error and uncertainty due to systematic error.**

This exercise of performing bias analysis with uncertainty in the bias
parameters is called probabilistic bias analysis. When running
`multibias_adjust()` with validation data (a `data_validation` object)
across bootstrap samples, the function automatically resamples from the Normal
distribution bias parameters (estimate and standard error) inferred from the
validation data. When running `multibias_adjust()` with specified bias
parameters (a `bias_params` object) across bootstrap samples, you must input the parameter values as statistical distributions (e.g., by using the `rnorm()` or
`runif()` function).

# How do I cite `multibias`?

asdf

# Does `multibias` support time-to-event data and analyses?

Not currently